score <- datIn[,ncol(datIn)]
fileOut <- file.path("~/AACR/SupervisedClass",questionName,"questionCrossValidation.Rdata")
load(fileOut)
classTot <- apply(predTot,1,which.max)
classTot
confusionMatrix(score,classTot)
levels(score) <- 1:3
confusionMatrix(score,classTot)
cross_validate
help(cross_validate)
dm <- create_matrix(datIn$Replication,removeNumbers = TRUE,removeSparseTerms = 0.998,stemWords = TRUE)
datCont <- create_container(dm,datIn$Score,trainSize=1:911,testSize=912:1011,virgin=FALSE)
models <- train_models(datCont,algorithms=c("SVM","SLDA","BOOSTING","BAGGING","RF","GLMNET","TREE","NNET","MAXENT"))
predicts <- classify_models(datCont,models)
votes1 <- apply(predicts[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
ind <- seq(1,18,2)
votes1 <- apply(predicts[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
confusionMatrix(datIn[912:1011,"Score"],votes1)
votes1
levels(votes1) <- 1:3
confusionMatrix(datIn[912:1011,"Score"],votes1)
votes1
levels
votes1
as.factor(votes1)
votes1 <- as.factor(votes1)
levels(votes1) <- 1:3
votes1
votes1 <- apply(predicts[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
votes1 <- as.factor(votes1)
votes1
levels(votes1) <- c(1,3,2)
votes1
confusionMatrix(datIn[912:1011,"Score"],votes1)
datIn[912:1011,"Score"]
votes1 <- apply(predicts[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
votes1 <- as.factor(votes1)
confusionMatrix(datIn[912:1011,"Score"],votes1)
votes1
levels
help(levels)
score <- datIn[912:1011,"Score"]
score
as.numeric(score)
confusionMatrix(score,votes1)
score
votes1
votes1 <- apply(predicts[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
confusionMatrix(score,votes1)
votes1 <- apply(predicts[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
confusionMatrix(score,votes1)
score <- as.numeric(datIn[912:1011,"Score"])
confusionMatrix(score,votes1)
score
votes1
votes1 <- apply(predicts[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
votes1
as.numeric(votes1)
as.numeric(as.factor(votes1))
votes1 <- apply(predicts[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
votes1[votes1=="one"] <- 1
votes1[votes1=="two"] <- 2
votes1[votes1=="three"] <- 3
votes1
as.numeric(votes1)
votes1 <- apply(predicts[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
votes1[votes1=="one"] <- 1
votes1[votes1=="two"] <- 2
votes1[votes1=="three"] <- 3
votes1 <- as.numeric(votes1)
score <- as.numeric(datIn[912:1011,"Score"])
confusionMatrix(score,votes1)
confusionMatrix(datIn[912:1011,"Score"],votes)
votes <- apply(predicts2[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
dm2 <- DocumentTermMatrix(Corpus(VectorSource(iconv(datIn$Replication,to="utf-8",sub=""))),control=list(removeNumbers = TRUE,stemWords = TRUE,tokenize=BigramTokenizer))
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
dm2 <- DocumentTermMatrix(Corpus(VectorSource(iconv(datIn$Replication,to="utf-8",sub=""))),control=list(removeNumbers = TRUE,stemWords = TRUE,tokenize=BigramTokenizer))
dm2b <- removeSparseTerms(dm2,0.99)
datCont2 <- create_container(dm2b,datIn$Score,trainSize=1:911,testSize=912:1011,virgin=FALSE)
models2 <- train_models(datCont2,algorithms=c("SVM","SLDA","BOOSTING","BAGGING","RF","GLMNET","TREE","NNET","MAXENT"))
predicts2 <- classify_models(datCont2,models2)
ind <- seq(1,18,2)
votes <- apply(predicts2[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
confusionMatrix(datIn[912:1011,"Score"],votes)
votes
datIn$Score
datIn$Score <- as.numeric(datIn$Score)
# create matrix
dm <- create_matrix(datIn$Replication,removeNumbers = TRUE,removeSparseTerms = 0.998,stemWords = TRUE)
#dm2 <- create_matrix(iconv(datIn$Replication,to="utf-8",sub=""),removeNumbers = TRUE,removeSparseTerms = 0.998,stemWords = TRUE,ngramLength=2)
dm2 <- DocumentTermMatrix(Corpus(VectorSource(iconv(datIn$Replication,to="utf-8",sub=""))),control=list(removeNumbers = TRUE,stemWords = TRUE,tokenize=BigramTokenizer))
dm2b <- removeSparseTerms(dm2,0.99)
# create container
datCont <- create_container(dm,datIn$Score,trainSize=1:911,testSize=912:1011,virgin=FALSE)
datCont2 <- create_container(dm2b,datIn$Score,trainSize=1:911,testSize=912:1011,virgin=FALSE)
#train models
models <- train_models(datCont,algorithms=c("SVM","SLDA","BOOSTING","BAGGING","RF","GLMNET","TREE","NNET","MAXENT"))
models2 <- train_models(datCont2,algorithms=c("SVM","SLDA","BOOSTING","BAGGING","RF","GLMNET","TREE","NNET","MAXENT"))
#classify models
predicts <- classify_models(datCont,models)
predicts2 <- classify_models(datCont2,models2)
ind <- seq(1,18,2)
votes <- apply(predicts2[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
confusionMatrix(datIn[912:1011,"Score"],votes)
votes1 <- apply(predicts[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
score <- as.numeric(datIn[912:1011,"Score"])
confusionMatrix(score,votes1)
dm1and2 <- do.call(tm::c.DocumentTermMatrix,list(dm1,dm2b))
dm1and2 <- do.call(tm::c.DocumentTermMatrix,list(dm,dm2b))
dm1and2 <- do.call(tm:::c.DocumentTermMatrix,list(dm,dm2b))
tm_combine
dm1and2 <-c(dm,dm2b)
help(NGramTokenizer)
BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
library(RWeka)
install.packages("Rweka")
install.packages("RWeka")
library(RWeka)
newBigramTokenizer = function(x) {
tokenizer1 = NGramTokenizer(x, Weka_control(min = 1, max = 2))
if (length(tokenizer1) != 0L) { return(tokenizer1)
} else return(WordTokenizer(x))
}
newBigramTokenizer("the lazy brown dog jumped over the quick gray fox")
dm2 <- DocumentTermMatrix(Corpus(VectorSource(iconv(datIn$Replication,to="utf-8",sub=""))),control=list(removeNumbers = TRUE,stemWords = TRUE,tokenize=newBigramTokenizer))
dm2b <- removeSparseTerms(dm2,0.99)
datCont2 <- create_container(dm2b,datIn$Score,trainSize=1:911,testSize=912:1011,virgin=FALSE)
models2 <- train_models(datCont2,algorithms=c("SVM","SLDA","BOOSTING","BAGGING","RF","GLMNET","TREE","NNET","MAXENT"))
predicts2 <- classify_models(datCont2,models2)
votes <- apply(predicts2[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
confusionMatrix(datIn[912:1011,"Score"],votes)
library(ggplot2)
help("facet_wrap")
p5 <- ggplot(datRC,aes(x=Responses,y=Accuracy))+
geom_point()+
facet_wrap(~RubricLevel,ncol=1)+
geom_errorbar(aes(ymin=AccuracyLower,ymax=AccuracyUpper))+
0
rl1 <- as.data.frame(confOutList[[1]])
rl2 <- as.data.frame(confOutList[[2]])
rl3 <- as.data.frame(confOutList[[3]])
rl1 <- cbind(rl1,data.frame(RubricLevel=rep("Correct Source",8)),Responses=round(seq(0.3,1.,0.1)*678))
rl2 <- cbind(rl2,data.frame(RubricLevel=rep("Source Soil Nutrients",8)),Responses=round(seq(0.3,1.,0.1)*678))
rl3 <- cbind(rl3,data.frame(RubricLevel=rep("Source Water",8)),Responses=round(seq(0.3,1.,0.1)*678))
datRC <- rbind(rl1,rl2,rl3)
load("~/afs/AACR/Users/steele24/ngramTesting/Root_CellCvRTT.Rdata")
rl1 <- as.data.frame(confOutList[[1]])
rl2 <- as.data.frame(confOutList[[2]])
rl3 <- as.data.frame(confOutList[[3]])
rl1 <- cbind(rl1,data.frame(RubricLevel=rep("Correct Source",8)),Responses=round(seq(0.3,1.,0.1)*678))
rl2 <- cbind(rl2,data.frame(RubricLevel=rep("Source Soil Nutrients",8)),Responses=round(seq(0.3,1.,0.1)*678))
rl3 <- cbind(rl3,data.frame(RubricLevel=rep("Source Water",8)),Responses=round(seq(0.3,1.,0.1)*678))
datRC <- rbind(rl1,rl2,rl3)
p5 <- ggplot(datRC,aes(x=Responses,y=Accuracy))+
geom_point()+
facet_wrap(~RubricLevel,ncol=1)+
geom_errorbar(aes(ymin=AccuracyLower,ymax=AccuracyUpper))
print(p5)
help("geom_point")
update.packages()
library(RTextTools)
library(XLConnect)
library(tm)
library(RWeka)
fileIn <-"~/AACR/Supervised/ReplicationTAandScore.xlsx"
newBigramTokenizer = function(x) {
tokenizer1 = NGramTokenizer(x, Weka_control(min = 1, max = 2))
if (length(tokenizer1) != 0L) { return(tokenizer1)
} else return(WordTokenizer(x))
}
wkbk <- loadWorkbook(fileIn)               #load Excel file
nSheets <- length(getSheets(wkbk))         #count number of sheets
datIn <- readWorksheet(wkbk,sheet=nSheets) #load last worksheet to table
if (length(datIn) == 0){datIn <- readWorksheet(wkbk,sheet=1)} #load worksheet to table
dm2 <- DocumentTermMatrix(Corpus(VectorSource(iconv(datIn$Replication,to="utf-8",sub=""))),control=list(removeNumbers = TRUE,stemWords = TRUE,tokenize=newBigramTokenizer))
dm2b <- removeSparseTerms(dm2,0.99)
datCont2 <- create_container(dm2b,datIn$Score,trainSize=1:911,testSize=912:1011,virgin=FALSE)
models2 <- train_models(datCont2,algorithms=c("SVM","SLDA","BOOSTING","BAGGING","RF","GLMNET","TREE","NNET","MAXENT"))
predicts2 <- classify_models(datCont2,models2)
ind <- seq(1,18,2)
votes <- apply(predicts2[,ind],1,function(x) names(sort(table(x),decreasing=TRUE)[1]))
votes
dm2b
inspect(dm2b)
inspect(dm2b[,1])
head(inspect(dm2b[,1]))
dm2b[,1]
inspect(dm2b[912:1011,1]
)
table(inspect(dm2b[912:1011,1]),datIn[912:1011,"Score"])
help(inspect)
as.vector(dm2b[912:1011,1])
table(as.vector(dm2b[912:1011,1]),datIn[912:1011,"Score"])
table(as.vector(dm2b[912:1011,1]),datIn[912:1011,"Score"]==1)
library(abd)
junk <- table(as.vector(dm2b[912:1011,1]),datIn[912:1011,"Score"]==1)
junk[1,1]*junk[2,2]/(junk[1,2]*junk[2,1])
fisher.test(junk)
trash <- as.vector(dm2b[912:1011,1])
trash==1
sum(trash)
index <- datIn[912:1011,"Score"]==1
index
trash[index]
sum(trash[index]==1)*length(trash)/(sum(trash[index])*sum(trash==1))
sum(trash[index]==1)
sum(trash[index]==1)*length(trash)/(sum(index)*sum(trash==1))
sum(trash[index]==1)*length(trash)/(sum(index)*sum(trash==1))-1
trash
sum(index)
sum(trash=1)
odd1 <- sum(trash[index]==1)/sum(index)
odd2 <- sum(trash=1)/length(trash)
oodd1
odd1
odd2
(odd1-odd2)/odd2
trash
junk <- dm2b[1:100,1:10]
juk
junk
inspect(junk)
cbind(junk,datIn[1:100,"Score"])
junk2 <- cbind(junk,datIn[1:100,"Score"])
inspect(junk2)
junk2
inspect(junk2)
calcOA <- function(terms,scores,testScore){
index <- scores==testScore
out <- sum(terms[index])*length(scores)/(sum(index)*sum(terms))-1
}
a <- trash
a
b <- datIn[1:100,"Score"]
calcOA(a,b,1)
calcOA <- function(terms,scores,testScore){
index <- scores==testScore
out <- sum(terms[index])*length(scores)/(sum(index)*sum(terms))-1
out
}
calcOA(a,b,1)
calcOA(a,b,2)
calcOA(a,b,3)
a <- trash
calcOA <- function(termIn,scoresIn,testScore){
index <- scoresIn==testScore
out <- sum(termsIn[index])*length(scoresIn)/(sum(index)*sum(termsIn))-1
out
}
termsIn <- trash
scores
b
scoresIn <- b
testScore <- 1
index <- scoresIn==testScore
sum(termsIn[index])
termsIn[index]
length(scoresIn)
(sum(index)
)
sum(termsIn)
calcOA <- function(termsIn,scoresIn,testScore){
index <- scoresIn==testScore
out <- sum(termsIn[index])*length(scoresIn)/(sum(index)*sum(termsIn))-1
out
}
calcOA(termsIn,scoresIn,1)
calcOA(termsIn,scoresIn,2)
calcOA(termsIn,scoresIn,3)
testScore <- 3
index <- scoresIn==testScore
sum(termsIn[index])
length(scoresIn)
(sum(index)
)
sum(termsIn)
sum(termsIn[index])/sum(index)
sum(termsIn)/length(scoresIn)
dtmIn <- dm2b
scoresIn <- datIn[,"Scores"]
scoresIn <- datIn[,"Score"]
out <- numeric()
for(i in seq(length(unique(scoresIn)))){
out <- rbind(out,apply(x,1,function(x) calcOA(x,scoresIn,i)))
}
help(apply)
for(i in seq(length(unique(scoresIn)))){
out <- rbind(out,apply(dtmIn,2,function(x) calcOA(x,scoresIn,i)))
}
out
out[1,]
sort(out[1,])
help(sort)
sort(out[1,],decreasing=TRUE)
sort(out[1,],decreasing=TRUE)[1:5]
names(sort(out[1,],decreasing=TRUE)[1:5])
mostOverabundant <- function(dtmIn,scoresIn,nMost){
#find the n most overabundant terms for each score
oAbun <- overAbundance(dtmIn,scoresIn)
out <- apply(oAbun,1,function(x) sort(x,decreasing=TRUE)[1:nMost])
}
mostOverabundant(dtmIn,scoresIn,5)
overAbundance <- function(dtmIn,scoresIn){
#         Calculate overabundance of ngram terms based on scoring
#         dtmIn - input document term matrix
#         scoreIn - input scoring assignment vectors
#mergedDat <- cbind(dtmIn,scoreIn)
out <- numeric()
for(i in seq(length(unique(scoresIn)))){
out <- rbind(out,apply(dtmIn,2,function(x) calcOA(x,scoresIn,i)))
}
out
}
mostOverabundant(dtmIn,scoresIn,5)
mostOverabundant <- function(dtmIn,scoresIn,nMost){
#find the n most overabundant terms for each score
oAbun <- overAbundance(dtmIn,scoresIn)
out <- apply(oAbun,1,function(x) sort(x,decreasing=TRUE)[1:nMost])
out
}
mostOverabundant(dtmIn,scoresIn,5)
names(sort(out[2,],decreasing=TRUE)[1:5])
sort(out[2,],decreasing=TRUE)[1:5]
sort(out[3,],decreasing=TRUE)[1:5]
mostOverabundant <- function(dtmIn,scoresIn,nMost){
#find the n most overabundant terms for each score
oAbun <- overAbundance(dtmIn,scoresIn)
out <- sapply(oAbun,1,function(x) sort(x,decreasing=TRUE)[1:nMost])
out
}
mostOverabundant(dtmIn,scoresIn,5)
help("sapply")
mostOverabundant <- function(dtmIn,scoresIn,nMost){
#find the n most overabundant terms for each score
oAbun <- overAbundance(dtmIn,scoresIn)
out <- lapply(oAbun,1,function(x) sort(x,decreasing=TRUE)[1:nMost])
out
}
mostOverabundant(dtmIn,scoresIn,5)
mostOverabundant <- function(dtmIn,scoresIn,nMost){
#find the n most overabundant terms for each score
oAbun <- overAbundance(dtmIn,scoresIn)
out <- lapply(oAbun,function(x) sort(x,decreasing=TRUE)[1:nMost])
out
}
mostOverabundant(dtmIn,scoresIn,5)
help(apply)
mostOverabundant <- function(dtmIn,scoresIn,nMost){
#find the n most overabundant terms for each score
oAbun <- overAbundance(dtmIn,scoresIn)
out <- apply(oAbun,1,function(x) sort(x,decreasing=TRUE)[1:nMost])
out
}
mostOverabundant(dtmIn,scoresIn,5)
names(mostOverabundant(dtmIn,scoresIn,5))
list(5,6)
list(a=5,b=6)
mostOverabundant <- function(dtmIn,scoresIn,nMost){
#find the n most overabundant terms for each score
oAbun <- overAbundance(dtmIn,scoresIn)
outNames <- apply(oAbun,1,function(x) names(sort(x,decreasing=TRUE)[1:nMost]))
outValues <- apply(oAbun,1,function(x) sort(x,decreasing=TRUE)[1:nMost])
list(names=outNames,values=outValues)
}
mostOverabundant(dtmIn,scoresIn,5)
mostOverabundant(dtmIn,scoresIn,10)
source('~/afs/AACR/Users/steele24/ngramTesting/crossValidateRTT.R')
source('~/afs/AACR/Users/steele24/ngramTesting/crossValidateRTT.R')
modelObjectFile <- "~/AACR/SupervisedClass/DietFriend/trainedModels.Rdata"
load(modelObjectFile)
nClasses <- 4
readDataIn <- function(fileIn){
#------------------ Read in data ------------------------------------------------
wkbk <- loadWorkbook(fileIn$datapath)               #load Excel file
#nSheets <- length(getSheets(wkbk))         #count number of sheets
datIn <- readWorksheet(wkbk,sheet=1) #load last worksheet to table
#if (length(datIn) == 0){datIn <- readWorksheet(wkbk,sheet=1)} #load worksheet to table
datIn <- datIn[complete.cases(datIn),] # remove NA entries
if (exists("datIn$Score")){datIn$Score <- as.numeric(datIn$Score)}
datIn
}
datIn <- readDataIn("~/afs/AACR/Projects/Feedback Reports/Diet_rpt/Diet_UMaine_Pelletreau_PRE_13Oct15.xls")
library(XLConnect)
datIn <- readDataIn("~/afs/AACR/Projects/Feedback Reports/Diet_rpt/Diet_UMaine_Pelletreau_PRE_13Oct15.xls")
readDataIn <- function(fileIn){
#------------------ Read in data ------------------------------------------------
wkbk <- loadWorkbook(fileIn)               #load Excel file
#nSheets <- length(getSheets(wkbk))         #count number of sheets
datIn <- readWorksheet(wkbk,sheet=1) #load last worksheet to table
#if (length(datIn) == 0){datIn <- readWorksheet(wkbk,sheet=1)} #load worksheet to table
datIn <- datIn[complete.cases(datIn),] # remove NA entries
if (exists("datIn$Score")){datIn$Score <- as.numeric(datIn$Score)}
datIn
}
datIn <- readDataIn("~/afs/AACR/Projects/Feedback Reports/Diet_rpt/Diet_UMaine_Pelletreau_PRE_13Oct15.xls")
catIndex <- 3:48
class(modelGW)=="list"
nClasses <- length(modelGW)
outDF <- numeric()
#==================== Make Predictions =======================================
for(j in seq(nClasses)){
#---------------- Stepwise LDA ----------------------------------------------
predGW <- predict(modelGW[[j]],datIn[,colSig[[j]]])$posterior
#---------------- Support Vector Machines -----------------------------------
predSVM <- predict(modelSVM[[j]],datIn[,catIndex],type="probabilities")
#---------------- Regularized DA ---------------------------------------------
predRDA <- predict(modelRDA[[j]],datIn[,catIndex])$posterior
#---------------- Penalized Logistic Regression ------------------------------
#predPLR <- predict(modelPLR[[j]],datIn[,catIndex])
tmp <- predict(modelPLR[[j]],datIn[,catIndex])
predPLR <- cbind(1-tmp,tmp)
#-------------------- Random Forrest --------------------------------------------
predRF <- predict(modelRF[[j]],datIn[,catIndex],type="prob")
#------------------- Boosted Trees -----------------------------------------------
predBT <- predict(modelBT[[j]],datIn[,catIndex],type="prob")
#-------------------- Logit Boost ----------------------------------------------------
predLB <- predict(modelLB[[j]],datIn[,catIndex],type="raw")
#-------------------Penalized Multinomial Regression ----------------------------
predPMR <- predict(modelPMR[[j]],datIn[,catIndex],type="prob")
#---------------------- multilayer perceptron ----------------------------------------------------------
predNN <- predict(modelNN[[j]],datIn[,catIndex],type="prob")
#======================== sythetic classifier ==================================================
predTot <- predGW+predSVM+predRDA+predPLR+predRF+predBT+predLB+predPMR+predNN
outDF <- cbind(outDF,apply(predTot,1,which.max)-1)
outDF <- cbind(outDF,apply(predTot,1,function(x) max(x)/sum(x)))
}
library(MASS)
library(penalizedLDA)
library(caret)
library(penalized)
library(klaR)
library(caTools)
library(kernlab)
library(randomForest)
nClasses <- length(modelGW)
outDF <- numeric()
#==================== Make Predictions =======================================
for(j in seq(nClasses)){
#---------------- Stepwise LDA ----------------------------------------------
predGW <- predict(modelGW[[j]],datIn[,colSig[[j]]])$posterior
#---------------- Support Vector Machines -----------------------------------
predSVM <- predict(modelSVM[[j]],datIn[,catIndex],type="probabilities")
#---------------- Regularized DA ---------------------------------------------
predRDA <- predict(modelRDA[[j]],datIn[,catIndex])$posterior
#---------------- Penalized Logistic Regression ------------------------------
#predPLR <- predict(modelPLR[[j]],datIn[,catIndex])
tmp <- predict(modelPLR[[j]],datIn[,catIndex])
predPLR <- cbind(1-tmp,tmp)
#-------------------- Random Forrest --------------------------------------------
predRF <- predict(modelRF[[j]],datIn[,catIndex],type="prob")
#------------------- Boosted Trees -----------------------------------------------
predBT <- predict(modelBT[[j]],datIn[,catIndex],type="prob")
#-------------------- Logit Boost ----------------------------------------------------
predLB <- predict(modelLB[[j]],datIn[,catIndex],type="raw")
#-------------------Penalized Multinomial Regression ----------------------------
predPMR <- predict(modelPMR[[j]],datIn[,catIndex],type="prob")
#---------------------- multilayer perceptron ----------------------------------------------------------
predNN <- predict(modelNN[[j]],datIn[,catIndex],type="prob")
#======================== sythetic classifier ==================================================
predTot <- predGW+predSVM+predRDA+predPLR+predRF+predBT+predLB+predPMR+predNN
outDF <- cbind(outDF,apply(predTot,1,which.max)-1)
outDF <- cbind(outDF,apply(predTot,1,function(x) max(x)/sum(x)))
}
predTot
datIn
rowSums(datIn[,catIndex])
j
predTot
outDF
predGW
predSVM
predSVM
predRDA
predPLR
predRF
predBT
predLB
predPMR
predNN
predRDA
junk <- predRDA
junk
is.na(junk)
junk[is.na(junk)] <- 0
junk
help(format)
format(data.frame(a=(1:5)*0.12342,b=(2:6)*0.234321))
format(data.frame(a=(1:5)*0.12342,b=(2:6)*0.234321),digits=2)
format(data.frame(a=(1:5)*3.12342,b=(2:6)*9.234321),digits=2)
format(data.frame(a=(1:5)*3.12342,b=(2:6)*9.234321),digits=0)
round(data.frame(a=(1:5)*3.12342,b=(2:6)*9.234321),digits=0)
help(xtable)
round(data.frame(a=(1:5)*3.12342,b=(2:6)*9.234321,c=c("adsfa","asdfwe","lkjoihj","ohklsd")),digits=0)
round(data.frame(a=(1:5)*3.12342,b=(2:6)*9.234321,c=c("adsfa","asdfwe","lkjoihj","ohklsd","ohjasodf")),digits=0)
format(data.frame(a=(1:5)*3.12342,b=(2:6)*9.234321,c=c("adsfa","asdfwe","lkjoihj","ohklsd","ohjasodf")),digits=0)
1:3
rev(1:3)
shiny::runApp('AACR/ExploratoryApp')
shiny::runApp('AACR/AACRQuestionSetup')
junk <- list()
junk[1] <- 1:5
junk[[1]] <- 1:5
junk[[2]] <- 1:5
junk
junk[[3]] <- "cat"
junk
shiny::runApp('AACR/AACRQuestionSetup')
seq(1,1)
seq(1,2)
library(slidify)
setwd("~/Documents/Talks/NARST/NARST2016v2/")
slidify("index.Rmd")
